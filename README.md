# ä¸€ã€Reference

> 1. [GitHub - ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B?tab=readme-ov-file)
> 2. [ChatGLM+LangChainå®è·µ](https://www.bilibili.com/video/BV1t8411y7fp?p=1)
> 3. [ChatGLM+LangChainå®è·µ  GitHubé¡¹ç›®ä»£ç ](https://github.com/IronSpiderMan/MachineLearningPractice)
> 4. [ChatGLM+LangChainå®˜æ–¹æ¡†æ¶è®²è§£](https://www.bilibili.com/video/BV13M4y1e7cN/?spm_id_from=333.788.recommend_more_video.5&vd_source=a2b906f1078e767936dd0bbcf1275e2e)



æ¨èä¸¤ä¸ªå›½äº§LLMæ™ºèƒ½åŠ©æ‰‹ï¼Œå¤§å®¶å¦‚æœå…¶ä¸­ä»£ç æœ‰ä¸æ‡‚çš„ç›´æ¥é—®LLMã€‚

> 1. [kimi](https://kimi.moonshot.cn/)
> 2. [é€šä¹‰åƒé—® ](https://tongyi.aliyun.com/qianwen/?spm=5176.28326591.0.0.40f713f4hDPj4r)





# äºŒã€æœ¬åœ°ç¯å¢ƒå‡†å¤‡

â€‹	å®‰è£… [Anaconda](https://www.anaconda.com/download/)ï¼Œå…·ä½“æ“ä½œå‚è€ƒæ–‡æ¡£ Python æ·±åº¦å­¦ä¹ ï¼šå®‰è£… Anaconda ä¸ PyTorchï¼ˆGPU ç‰ˆï¼‰åº“.pdfã€‚æ–‡ä»¶æ¥æºäºbç«™è§†é¢‘[Pythonæ·±åº¦å­¦ä¹ ï¼šå®‰è£…Anacondaã€PyTorchï¼ˆGPUç‰ˆï¼‰åº“ä¸PyCharm_å“”å“©å“”å“©_bilibili](https://www.bilibili.com/video/BV1cD4y1H7Tk/?spm_id_from=333.337.search-card.all.click&vd_source=a2b906f1078e767936dd0bbcf1275e2e)





### 2.1è™šæ‹Ÿç¯å¢ƒ

è¿›å…¥åˆšåˆšå®‰è£…çš„anaconda Powershell Prompt

```cmd
# è™šæ‹Ÿç¯å¢ƒåˆ—è¡¨
conda env list

# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒChatBot
conda create -n ChatBot python=3.10

# åˆ é™¤è™šæ‹Ÿç¯å¢ƒChatBot
# conda remove -n ChatBot --all

# è¿›å…¥/ç¦»å¼€è™šæ‹Ÿç¯å¢ƒï¼ˆå¦‚æœéœ€è¦å®‰è£…åº“ï¼Œè¿›å…¥å¯¹åº”è™šæ‹Ÿç¯å¢ƒå®‰è£…ï¼‰
conda activate ChatBot  / conda deactivate

# åˆ—å‡º Jupyter çš„è™šæ‹Ÿç¯å¢ƒè¿æ¥åˆ—è¡¨ï¼Œ
jupyter kernelspec list

# å®‰è£… ipykernel
pip install -i https://pypi.tuna.tsinghua.edu.cn/simple ipykernel

# å°†è™šæ‹Ÿç¯å¢ƒå¯¼å…¥ Jupyter çš„ kernel ä¸­
python -m ipykernel install --user --name=ChatBot

# åˆ é™¤å¯¹åº”è¿æ¥
jupyter kernelspec remove ChatBot 

#æ˜¾å¡ä¿¡æ¯ 
nvidia -smi
```



### 2.2é¡¹ç›®ç¯å¢ƒå®‰è£…

```cmd
# torch-GPU
pip install torch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0 --index-url https://download.pytorch.org/whl/cu121

# å®‰è£…requirementsç¯å¢ƒ
pip install -r requirements.txt
```





### 2.3æ–‡ä»¶ä¸‹è½½

```cmd
# è¿›å…¥Eç›˜programæ–‡ä»¶å¤¹ä¸­
cd E:\Jupyter\Program\

# å»ºç«‹é¡¹ç›®æ–‡ä»¶å¤¹ChatGLM-LangChain
mkdir ChatGLM-LangChain

# è¿›å…¥é¡¹ç›®æ–‡ä»¶å¤¹
cd ChatGLM-LangChain

# ä¸‹è½½æ¨¡å‹å‚æ•°chatglm-6b-int4ï¼Œå¯ä»¥è¿›å…¥å¯¹åº”urlæŸ¥çœ‹æ¨¡å‹
git clone https://huggingface.co/THUDM/chatglm-6b-int4

# æ˜¾å­˜å¤§å¯ä»¥ä¸‹è½½æ¨¡å‹chatglm-6bï¼Œ
git clone https://huggingface.co/THUDM/chatglm-6b
```



â€‹	å®‰è£…cuda toolkit[CUDA Toolkit Archive | NVIDIA Developer](https://developer.nvidia.com/cuda-toolkit-archive)ã€‚å¦‚æœä¸å®‰è£…çš„è¯chatglm-6b-int4æ¨¡å‹ä¼šæŠ¥é”™ï¼Œä»¥åŠchatglm-6b.quantize(4)ä¹Ÿä¼šæŠ¥é”™ã€‚



```cmd
#win+rç„¶åè¾“å…¥cmdè¿›å…¥å‘½ä»¤è¡Œ
# è¾“å…¥ä»¥ä¸‹å‘½ä»¤è¿›å…¥jupyter notebook
jupyter lab
```



# ä¸‰ã€é¡¹ç›®ä»£ç Step

â€‹	æ–°å»ºjupyter notebookæ–‡ä»¶ï¼Œå‘½åä¸ºChaBotã€‚ç¯å¢ƒä¸ºå¼€å§‹åˆ›å»ºçš„è™šæ‹Ÿç¯å¢ƒChaBotã€‚

### 3.1æµ‹è¯•æ¨¡å‹

```python
import warnings
warnings.filterwarnings("ignore")

# åŠ è½½æ¨¡å‹
from transformers import AutoTokenizer, AutoModel
tokenizer = AutoTokenizer.from_pretrained("./chatglm-6b-int4", trust_remote_code=True)
model = AutoModel.from_pretrained("./chatglm-6b-int4", trust_remote_code=True).half().cuda()
model = model.eval()

# æµ‹è¯•è¾“å…¥
response, history = model.chat(tokenizer, "ä½ å¥½", history=[])
print(response)
response, history = model.chat(tokenizer, "æ™šä¸Šç¡ä¸ç€åº”è¯¥æ€ä¹ˆåŠ", history=history)
print(response)
```





### API

â€‹	å•ç‹¬åœ¨åŒä¸€ç›®å½•å»ºç«‹pyæ–‡ä»¶api.py

```python
from fastapi import FastAPI, Request
from transformers import AutoTokenizer, AutoModel
import uvicorn, json, datetime
import torch

DEVICE = "cuda"
DEVICE_ID = "0"
CUDA_DEVICE = f"{DEVICE}:{DEVICE_ID}" if DEVICE_ID else DEVICE


def torch_gc():
    if torch.cuda.is_available():
        with torch.cuda.device(CUDA_DEVICE):
            torch.cuda.empty_cache()
            torch.cuda.ipc_collect()


app = FastAPI()


@app.post("/")
async def create_item(request: Request):
    global model, tokenizer
    json_post_raw = await request.json()
    json_post = json.dumps(json_post_raw)
    json_post_list = json.loads(json_post)
    prompt = json_post_list.get('prompt')
    history = json_post_list.get('history')
    max_length = json_post_list.get('max_length')
    top_p = json_post_list.get('top_p')
    temperature = json_post_list.get('temperature')
    response, history = model.chat(tokenizer,
                                   prompt,
                                   history=history,
                                   max_length=max_length if max_length else 2048,
                                   top_p=top_p if top_p else 0.7,
                                   temperature=temperature if temperature else 0.95)
    now = datetime.datetime.now()
    time = now.strftime("%Y-%m-%d %H:%M:%S")
    answer = {
        "response": response,
        "history": history,
        "status": 200,
        "time": time
    }
    log = "[" + time + "] " + '", prompt:"' + prompt + '", response:"' + repr(response) + '"'
    print(log)
    torch_gc()
    return answer


if __name__ == '__main__':
    tokenizer = AutoTokenizer.from_pretrained("./chatglm-6b-int4", trust_remote_code=True)
    model = AutoModel.from_pretrained("./chatglm-6b-int4", trust_remote_code=True).half().cuda()
    model.eval()
    uvicorn.run(app, host='0.0.0.0', port=8000, workers=1)
```

â€‹	ç„¶å

```python
# æ­¤æ—¶æœ‰ä¸¤ç§æ–¹æ³•è¿è¡Œapi.pyæ–‡ä»¶

#1.åœ¨jupyter labä¸­æ‰“å¼€å‘½ä»¤è¡Œ
#2.é€šè¿‡Anaconda powershell promptè¿›å…¥å‘½ä»¤è¡Œï¼ˆä¸æ˜¯win+r cmdçš„ï¼‰ã€‚conda activate ChatBotè¿›å…¥ChatBotè™šæ‹Ÿç¯å¢ƒï¼Œcd E:\Jupyter\Program\ChatGLM-LangChainè¿›å…¥é¡¹ç›®æ–‡ä»¶å¤¹

# è¿è¡Œå‰é¢åˆ›å»ºçš„api.py
python api.py
```



### APIæµ‹è¯•

â€‹	è¿›å…¥ChatBot.ipynpè¾“å…¥ä»¥ä¸‹ä»£ç æµ‹è¯•api

```python
import requests
def chat(prompt, history) :
    resp = requests .post(
        url = 'http://127.0.0.1:8000',
        json = {"prompt": prompt,"history": history},
        headers = {"Content-Type": "application/json;charset=utf-8"}
    )
    return resp.json()['response'], resp.json()['history']
             
history = []
while True:
    response, history = chat(input("Question:"), history)
    print('Answer:', response)
```



### æ–‡æ¡£åˆ’åˆ†

```python
from langchain.document_loaders import DirectoryLoader, UnstructuredFileLoader
from langchain.text_splitter import CharacterTextSplitter
def load_documents(directory="./Books/"):
    """
    åŠ è½½booksä¸‹çš„æ–‡ä»¶ï¼Œè¿›è¡Œæ‹†åˆ†
    :param directory:
    :return:
    """
    loader = DirectoryLoader(directory)
    documents = loader.load()
    text_spliter = CharacterTextSplitter(chunk_size=35, chunk_overlap=10)
    split_docs = text_spliter.split_documents(documents)
    return split_docs
split_docs = load_documents()
split_docs
```



### åŠ è½½Embeddingæ¨¡å‹

```python
from langchain.embeddings.huggingface import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
embedding_model_dict = {
    "ernie-tiny": "nghuyong/ernie-3.0-nano-zh",
    "ernie-base": "nghuyong/ernie-3.0-base-zh",
    "text2vec": "GanymedeNil/text2vec-large-chinese",
    "text2vec2": "uer/sbert-base-chinese-nli",
    "text2vec3": "shibing624/text2vec-base-chinese",
}


def load_embedding_model(model_name="text2vec3"):
    """
    åŠ è½½embeddingæ¨¡å‹
    :param model_name:
    :return:
    """
    encode_kwargs = {"normalize_embeddings": False}
    model_kwargs = {"device": "cuda:0"}
    return HuggingFaceEmbeddings(
        model_name=embedding_model_dict[model_name],
        model_kwargs=model_kwargs,
        encode_kwargs=encode_kwargs
    )
embeddings = load_embedding_model()
embeddings
```



### å‘é‡åº“chrome

```python
from langchain.vectorstores import Chroma
import os
def store_chroma(docs, embeddings, persist_directory="VectorStore"):
    """
    è®²æ–‡æ¡£å‘é‡åŒ–ï¼Œå­˜å…¥å‘é‡æ•°æ®åº“
    :param docs:
    :param embeddings:
    :param persist_directory:
    :return:
    """
    db = Chroma.from_documents(docs, embeddings, persist_directory=persist_directory)
    db.persist()
    return db

# åŠ è½½æ•°æ®åº“
if not os.path.exists('VectorStore'):
    split_docs = load_documents()
    db = store_chroma(split_docs, embeddings)
else:
    db = Chroma(persist_directory='VectorStore', embedding_function=embeddings)
```



### Prompt_QA

```python
from langchain.llms import ChatGLM
# åˆ›å»ºllm
llm = ChatGLM(
    endpoint_url='http://127.0.0.1:8000',
    max_token=80000,
    top_p=0.9
)

from langchain.prompts import PromptTemplate
QA_CHAIN_PROMPT = PromptTemplate.from_template("""æ ¹æ®ä¸‹é¢çš„ä¸Šä¸‹æ–‡ï¼ˆcontextï¼‰å†…å®¹å›ç­”é—®é¢˜ã€‚
å¦‚æœä½ ä¸çŸ¥é“ç­”æ¡ˆï¼Œå°±å›ç­”ä¸çŸ¥é“ï¼Œä¸è¦è¯•å›¾ç¼–é€ ç­”æ¡ˆã€‚ç­”æ¡ˆæœ€å¤š3å¥è¯ï¼Œä¿æŒç­”æ¡ˆç®€ä»‹ã€‚
æ€»æ˜¯åœ¨ç­”æ¡ˆç»“æŸæ—¶è¯´â€è°¢è°¢ä½ çš„æé—®ï¼â€œ{context}
é—®é¢˜ï¼š{question}""")
print(QA_CHAIN_PROMPT)

from langchain.chains import RetrievalQA

# search_kwargs={"k": 1}è®¾è®¡æ£€ç´¢çš„æ–‡æ¡£æ•°é‡
retriever = db.as_retriever(search_kwargs={"k": 1}) 
qa = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever,
    verbose=True,
    chain_type_kwargs={"prompt": QA_CHAIN_PROMPT}
)


```



### Test

```python
qa.run('å¯ä»¥ç¬‘ä¸€ä¸ªå—')
```





### Gradio_UI



```python
import gradio as gr
import time
def chat(question, history):
    response = qa.run(question)
    return response
demo = gr.ChatInterface(chat)
demo.launch(inbrowser=True)
```



æ¢æˆä»¥ä¸‹ä»£ç å¢åŠ ä¸Šä¼ æ–‡æ¡£åŠŸèƒ½

```python
# å¢åŠ ä¸Šä¼ æ–‡æ¡£åŠŸèƒ½
# https://www.gradio.app/docs/chatbot

import gradio as gr
import time
def add_text(history, text):
    history = history + [(text, None)]
    return history, gr.update(value="", interactive=False)


def add_file(history, file):
    """
    ä¸Šä¼ æ–‡ä»¶åçš„å›è°ƒå‡½æ•°ï¼Œå°†ä¸Šä¼ çš„æ–‡ä»¶å‘é‡åŒ–å­˜å…¥æ•°æ®åº“
    :param history:
    :param file:
    :return:
    """
    global qa
    directory = os.path.dirname(file.name)
    documents = load_documents(directory)
    db = store_chroma(documents, embeddings)
    retriever = db.as_retriever()
    qa.retriever = retriever
    history = history + [((file.name,), None)]
    return history


def bot(history):
    """
    èŠå¤©è°ƒç”¨çš„å‡½æ•°
    :param history:
    :return:
    """
    message = history[-1][0]
    if isinstance(message, tuple):
        response = "æ–‡ä»¶ä¸Šä¼ æˆåŠŸï¼ï¼"
    else:
        response = qa({"query": message})['result']
    history[-1][1] = ""
    for character in response:
        history[-1][1] += character
        time.sleep(0.05)
        yield history


with gr.Blocks() as demo:
    chatbot = gr.Chatbot(
        [],
        elem_id="chatbot",
        bubble_full_width=False,
        avatar_images=(None, (os.path.join(os.path.dirname('__file__'), "avatar.png"))),
    )

    with gr.Row():
        txt = gr.Textbox(
            scale=4,
            show_label=False,
            placeholder="Enter text and press enter, or upload an image",
            container=False,
        )
        btn = gr.UploadButton("ğŸ“", file_types=['txt'])

    txt_msg = txt.submit(add_text, [chatbot, txt], [chatbot, txt], queue=False).then(
        bot, chatbot, chatbot
    )
    txt_msg.then(lambda: gr.update(interactive=True), None, [txt], queue=False)
    file_msg = btn.upload(add_file, [chatbot, btn], [chatbot], queue=False).then(
        bot, chatbot, chatbot
    )

demo.queue()
if __name__ == "__main__":
    demo.launch()
```





